# MineContext Environment Configuration
# Copy this file to .env and fill in your actual values
# DO NOT commit .env file to version control!

# ============================================
# LLM Configuration (Vision Language Model)
# ============================================

# Provider: openai, doubao, ollama, localai, llamacpp, custom
LLM_PROVIDER=ollama

# Model ID - the specific model you want to use
# Examples:
#   Ollama: qwen2.5:14b, llama3.1, mistral, gemma2:9b
#   OpenAI: gpt-4o, gpt-4-turbo, gpt-3.5-turbo
#   Doubao: doubao-seed-1-6-flash-250828
LLM_MODEL=qwen2.5:14b

# Base URL for the LLM API
# Examples:
#   Ollama: http://localhost:11434/v1
#   OpenAI: https://api.openai.com/v1
#   Doubao: https://ark.cn-beijing.volces.com/api/v3
#   LocalAI: http://localhost:8080/v1
LLM_BASE_URL=http://localhost:11434/v1

# API Key (leave empty for local providers like Ollama)
# For OpenAI: sk-...
# For Doubao: your-doubao-api-key
# For Ollama/LocalAI/LlamaCPP: leave empty or omit
LLM_API_KEY=

# ============================================
# Embedding Model Configuration
# ============================================

# Provider for embedding (can be different from LLM provider)
EMBEDDING_PROVIDER=ollama

# Embedding Model ID
# Examples:
#   Ollama: nomic-embed-text, mxbai-embed-large, bge-m3
#   OpenAI: text-embedding-3-large, text-embedding-3-small
#   Doubao: doubao-embedding
EMBEDDING_MODEL=nomic-embed-text

# Base URL for embedding API (if different from LLM)
# Leave empty to use the same as LLM_BASE_URL
EMBEDDING_BASE_URL=http://localhost:11434/v1

# API Key for embedding (if different from LLM)
# Leave empty to use the same as LLM_API_KEY
EMBEDDING_API_KEY=

# ============================================
# Optional: Advanced Settings
# ============================================

# Context path - where to store data
# CONTEXT_PATH=.

# API authentication key (for production deployments)
# CONTEXT_API_KEY=your-secure-api-key

# ============================================
# Web Server (Backend) Configuration
# Used by both backend (FastAPI/Uvicorn) and frontend (via Vite env)
# ============================================

# Host to bind the backend web server
WEB_HOST=127.0.0.1

# Port to bind the backend web server (frontend will default to this port in dev)
WEB_PORT=8000

# Note: Frontend reads Vite-prefixed vars. electron-vite is configured to map WEB_HOST/WEB_PORT
# into VITE_WEB_HOST/VITE_WEB_PORT automatically. Do NOT duplicate values.

# ============================================
# Example Configurations
# ============================================

# --- Ollama (Local, No API Key) ---
# LLM_PROVIDER=ollama
# LLM_MODEL=qwen2.5:14b
# LLM_BASE_URL=http://localhost:11434/v1
# LLM_API_KEY=
# EMBEDDING_PROVIDER=ollama
# EMBEDDING_MODEL=nomic-embed-text
# EMBEDDING_BASE_URL=http://localhost:11434/v1
# EMBEDDING_API_KEY=

# --- OpenAI ---
# LLM_PROVIDER=openai
# LLM_MODEL=gpt-4o
# LLM_BASE_URL=https://api.openai.com/v1
# LLM_API_KEY=sk-your-openai-api-key-here
# EMBEDDING_PROVIDER=openai
# EMBEDDING_MODEL=text-embedding-3-large
# EMBEDDING_BASE_URL=https://api.openai.com/v1
# EMBEDDING_API_KEY=sk-your-openai-api-key-here

# --- Mixed: OpenAI for LLM, Ollama for Embedding ---
# LLM_PROVIDER=openai
# LLM_MODEL=gpt-4o
# LLM_BASE_URL=https://api.openai.com/v1
# LLM_API_KEY=sk-your-openai-api-key-here
# EMBEDDING_PROVIDER=ollama
# EMBEDDING_MODEL=nomic-embed-text
# EMBEDDING_BASE_URL=http://localhost:11434/v1
# EMBEDDING_API_KEY=

# --- Doubao (Volcengine) ---
# LLM_PROVIDER=doubao
# LLM_MODEL=doubao-seed-1-6-flash-250828
# LLM_BASE_URL=https://ark.cn-beijing.volces.com/api/v3
# LLM_API_KEY=your-doubao-api-key
# EMBEDDING_PROVIDER=doubao
# EMBEDDING_MODEL=doubao-embedding
# EMBEDDING_BASE_URL=https://ark.cn-beijing.volces.com/api/v3
# EMBEDDING_API_KEY=your-doubao-api-key
